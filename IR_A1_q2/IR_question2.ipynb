{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_question2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#The database was uploaded to drive and the drive was mounted on google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BSqm0aEG75Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The location of the database\n",
        "database = \"/content/drive/MyDrive/Humor,Hist,Media,Food\"\n"
      ],
      "metadata": {
        "id": "m4rooTpnxNnS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports used\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "VVytCSjHOp-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the stop words\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "VS9UQYBxs-d2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function is used to preprocess the text\n",
        "def preprocess(text):\n",
        "    #Convert text to lower\n",
        "    text = text.lower()\n",
        "\n",
        "    #Remove punctuations\n",
        "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "    arr = tokenizer.tokenize(text)\n",
        "\n",
        "    #Remove the stop words\n",
        "    arr = [w for w in arr if not w in stop_words]\n",
        "\n",
        "    #The spaces are removed in tokenisation function\n",
        "\n",
        "    return arr"
      ],
      "metadata": {
        "id": "NlBGyIclcjZm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This creates positional index\n",
        "#Input are the gloabl dictionary \"pos_idx\",The tokenised \"arr\" and the current document number (doc_num)\n",
        "def createPositionalIndex(arr,pos_idx,doc_num):\n",
        "    for pos,term in enumerate(arr):\n",
        "        if term in pos_idx:\n",
        "            pos_idx[term][0] += 1                          #update the frequency of the word\n",
        "\n",
        "            if doc_num in pos_idx[term][1]:\n",
        "                pos_idx[term][1][doc_num].append(pos)      #if the doc_num of the word exist add the position\n",
        "            else:\n",
        "                temp_list = [pos]\n",
        "                pos_idx[term][1][doc_num] = temp_list      #if the doc_num of the word doestnt exist create it\n",
        "        else:\n",
        "            temp_list = [1,{doc_num: [pos]}]\n",
        "            pos_idx[term] = temp_list                      #if the term doesnt exist, initialise the frequency and the dictionary\n",
        "\n"
      ],
      "metadata": {
        "id": "nLkhh4B0JsuT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# assign directory\n",
        "directory = database\n",
        "pos_idx = {}            #This stores the positional index\n",
        "doc_num = 0\n",
        "file_dic = {}           #This stores the file names as value and doc_num as key\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    if os.path.isfile(f):\n",
        "\n",
        "        file1 = open(f,errors=\"ignore\") \n",
        "\n",
        "        text = file1.read()\n",
        "        \n",
        "        #Preprocess\n",
        "        arr = preprocess(text)\n",
        "\n",
        "        #Create positional index\n",
        "        createPositionalIndex(arr,pos_idx,doc_num)\n",
        "        \n",
        "        #Assign Doc_num to file name\n",
        "        file_dic[doc_num] = f\n",
        "        doc_num += 1\n",
        "\n",
        "        file1.close()"
      ],
      "metadata": {
        "id": "9kX-ACZ3HOjR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_idx"
      ],
      "metadata": {
        "id": "umY2kqW4pQ4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------Query--------------------------------#\n",
        "query = input()\n",
        "result = {}\n",
        "count = 0\n",
        "query_arr = preprocess(query)  #This processes the query\n",
        "def find_doc(query):\n",
        "    global result\n",
        "    global count\n",
        "    for word in query:\n",
        "        try:\n",
        "            current_dic = pos_idx[word][1]\n",
        "        except:\n",
        "            print(\"No doc found\")\n",
        "            break\n",
        "        if(len(result) == 0):\n",
        "            result = current_dic \n",
        "        else:\n",
        "            set_result = set(result.keys())\n",
        "            common_docs = set_result.intersection(set(current_dic.keys()))      #This is the intersection of docs between the final resultant \n",
        "            dic = {}                                                            #dictionary and the current selected word docs\n",
        "            for doc_id in common_docs:\n",
        "                dic[doc_id] = result[doc_id]\n",
        "            if(len(common_docs)) == 0:\n",
        "                print(\"No doc found\")\n",
        "                break\n",
        "            result = dic                                                        #This is the resultant based on the same docs\n",
        "            for doc_id in common_docs:\n",
        "                #iterate over the posting list\n",
        "                \n",
        "                inc_list = [x+count for x in result[doc_id]]\n",
        "                temp_pos = set(current_dic[doc_id]).intersection(set(inc_list)) #This finds the position in accordance to query\n",
        "            \n",
        "                if len(temp_pos) == 0:                                          #if No position is found for the current word in the doc, delete it\n",
        "                    del result[doc_id]\n",
        "            \n",
        "        count += 1\n",
        "\n",
        "\n",
        "\n",
        "find_doc(query_arr)             #Calling the function\n"
      ],
      "metadata": {
        "id": "s8KqPyRRMINj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "vl1nTaCs5pY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = []\n",
        "for key in result.keys():\n",
        "    print(key)\n",
        "    names.append(file_dic[key])\n"
      ],
      "metadata": {
        "id": "HPeq0TNdDWtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The number of docs found are \",len(names))"
      ],
      "metadata": {
        "id": "gm1r4pvbXKyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names"
      ],
      "metadata": {
        "id": "a_mj9PNMDnaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic"
      ],
      "metadata": {
        "id": "rZGwqJrlaSpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to pdf /content/drive/MyDrive/IR_question2.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grds7rsMaTJx",
        "outputId": "b39bf9ed-57a5-40b8-bf05-834bd6d6a0c8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/IR_question2.ipynb to pdf\n",
            "[NbConvertApp] Writing 35767 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: [u'bibtex', u'./notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 29740 bytes to /content/drive/MyDrive/IR_question2.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vEA00MSqapXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}